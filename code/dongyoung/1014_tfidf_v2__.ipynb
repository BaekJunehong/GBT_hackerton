{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝 모델 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석을 위한 pandas, 수치계산을 위한 numpy, 시각화를 위한 seaborn, matplotlib 을 로드합니다.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54314, 4), (23405, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 110\n",
    "\n",
    "# 학습, 예측 데이터셋을 불러옵니다.\n",
    "df_train = pd.read_csv(\"C:/Users/KimDongyoung/Desktop/GBT해커톤/GBT_hackerton/data/train_df_1012.csv\")\n",
    "df_test = pd.read_csv(\"C:/Users/KimDongyoung/Desktop/GBT해커톤/GBT_hackerton/data/test_df_1012.csv\")\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "분류\n",
       "지역          26850\n",
       "경제:부동산       3447\n",
       "사회:사건_사고     2545\n",
       "경제:반도체       2309\n",
       "사회:사회일반      1457\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['분류'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 로지스틱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_iter = 200 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF - Accuracy: 0.7945318972659486, Macro F1-score: 0.49684338301538167\n",
      "Count Vectorization - Accuracy: 0.8345760839547086, Macro F1-score: 0.6715446278197489\n",
      "Word2Vec (CBOW) - Accuracy: 0.7816441130442787, Macro F1-score: 0.539596364023002\n",
      "Word2Vec (Skip-gram) - Accuracy: 0.7753843321366105, Macro F1-score: 0.5078328918048641\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# 경고 메시지 억제\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 학습, 예측 데이터셋을 불러옵니다.\n",
    "df_train = pd.read_csv(\"C:/Users/KimDongyoung/Desktop/GBT해커톤/GBT_hackerton/data/train_df_1012.csv\")\n",
    "df_test = pd.read_csv(\"C:/Users/KimDongyoung/Desktop/GBT해커톤/GBT_hackerton/data/test_df_1012.csv\")\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# 텍스트와 레이블 분리\n",
    "X = df_train['키워드']  # 키워드 컬럼\n",
    "y = df_train['분류']  # 카테고리 컬럼\n",
    "\n",
    "# 데이터 분할 (클래스 비율 동일하게 유지)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_valid_encoded = label_encoder.transform(y_valid)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# Count Vectorization\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_valid_count = count_vectorizer.transform(X_valid)\n",
    "\n",
    "# Word2Vec 벡터화 (CBOW)\n",
    "w2v_model_cbow = Word2Vec(sentences=[text.split() for text in X_train], vector_size=256, window=10, min_count=2, workers=4, sg=0)\n",
    "X_train_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# Word2Vec 벡터화 (Skip-gram)\n",
    "w2v_model_sg = Word2Vec(sentences=[text.split() for text in X_train], vector_size=256, window=10, min_count=2, workers=4, sg=1)\n",
    "X_train_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# 로지스틱 회귀 모델 학습 및 평가 함수\n",
    "def train_and_evaluate(X_train, X_valid, y_train, y_valid, method_name):\n",
    "    model = LogisticRegression(max_iter=200, random_state=RANDOM_STATE)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "    macro_f1 = f1_score(y_valid, y_valid_pred, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "    # print(classification_report(y_valid, y_valid_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# TF-IDF\n",
    "train_and_evaluate(X_train_tfidf, X_valid_tfidf, y_train_encoded, y_valid_encoded, \"TF-IDF\")\n",
    "\n",
    "# Count Vectorization\n",
    "train_and_evaluate(X_train_count, X_valid_count, y_train_encoded, y_valid_encoded, \"Count Vectorization\")\n",
    "\n",
    "# Word2Vec (CBOW)\n",
    "train_and_evaluate(X_train_w2v_cbow, X_valid_w2v_cbow, y_train_encoded, y_valid_encoded, \"Word2Vec (CBOW)\")\n",
    "\n",
    "# Word2Vec (Skip-gram)\n",
    "train_and_evaluate(X_train_w2v_sg, X_valid_w2v_sg, y_train_encoded, y_valid_encoded, \"Word2Vec (Skip-gram)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF - Accuracy: 0.7949921752738655, Macro F1-score: 0.4995781881822617\n",
      "Count Vectorization - Accuracy: 0.8337475835404584, Macro F1-score: 0.6715901126181764\n",
      "Word2Vec (CBOW) - Accuracy: 0.7771333885666943, Macro F1-score: 0.5280927156285279\n",
      "Word2Vec (Skip-gram) - Accuracy: 0.7779618889809445, Macro F1-score: 0.5159025245549202\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀 모델 학습 및 평가 함수\n",
    "def train_and_evaluate(X_train, X_valid, y_train, y_valid, method_name):\n",
    "    model = LogisticRegression(max_iter=100, random_state=RANDOM_STATE)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "    macro_f1 = f1_score(y_valid, y_valid_pred, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "    # print(classification_report(y_valid, y_valid_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# TF-IDF\n",
    "train_and_evaluate(X_train_tfidf, X_valid_tfidf, y_train_encoded, y_valid_encoded, \"TF-IDF\")\n",
    "\n",
    "# Count Vectorization\n",
    "train_and_evaluate(X_train_count, X_valid_count, y_train_encoded, y_valid_encoded, \"Count Vectorization\")\n",
    "\n",
    "# Word2Vec (CBOW)\n",
    "train_and_evaluate(X_train_w2v_cbow, X_valid_w2v_cbow, y_train_encoded, y_valid_encoded, \"Word2Vec (CBOW)\")\n",
    "\n",
    "# Word2Vec (Skip-gram)\n",
    "train_and_evaluate(X_train_w2v_sg, X_valid_w2v_sg, y_train_encoded, y_valid_encoded, \"Word2Vec (Skip-gram)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 랜덤포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators=100 이 적절함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF - Accuracy: 0.7741876093160269, Macro F1-score: 0.5559152975472867\n",
      "Count Vectorization - Accuracy: 0.7740034981128602, Macro F1-score: 0.5602129538670343\n",
      "Word2Vec (CBOW) - Accuracy: 0.8024486790021172, Macro F1-score: 0.5950446513823952\n",
      "Word2Vec (Skip-gram) - Accuracy: 0.7979379545245329, Macro F1-score: 0.6043294228218642\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# 학습, 예측 데이터셋을 불러옵니다.\n",
    "df_train = pd.read_csv(\"C:/Users/KimDongyoung/Desktop/GBT해커톤/GBT_hackerton/data/train_df_1012.csv\")\n",
    "df_test = pd.read_csv(\"C:/Users/KimDongyoung/Desktop/GBT해커톤/GBT_hackerton/data/test_df_1012.csv\")\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# 텍스트와 레이블 분리\n",
    "X = df_train['키워드']  # 키워드 컬럼\n",
    "y = df_train['분류']  # 카테고리 컬럼\n",
    "\n",
    "# 데이터 분할 (클래스 비율 동일하게 유지)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_valid_encoded = label_encoder.transform(y_valid)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# Count Vectorization\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_valid_count = count_vectorizer.transform(X_valid)\n",
    "\n",
    "# Word2Vec 벡터화 (CBOW)\n",
    "w2v_model_cbow = Word2Vec(sentences=[text.split() for text in X_train], vector_size=256, window=10, min_count=2, workers=4, sg=0)\n",
    "X_train_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# Word2Vec 벡터화 (Skip-gram)\n",
    "w2v_model_sg = Word2Vec(sentences=[text.split() for text in X_train], vector_size=256, window=10, min_count=2, workers=4, sg=1)\n",
    "X_train_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# 랜덤 포레스트 모델 학습 및 평가 함수\n",
    "def train_and_evaluate(X_train, X_valid, y_train, y_valid, method_name):\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "    macro_f1 = f1_score(y_valid, y_valid_pred, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "    # print(classification_report(y_valid, y_valid_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# TF-IDF\n",
    "train_and_evaluate(X_train_tfidf, X_valid_tfidf, y_train_encoded, y_valid_encoded, \"TF-IDF\")\n",
    "\n",
    "# Count Vectorization\n",
    "train_and_evaluate(X_train_count, X_valid_count, y_train_encoded, y_valid_encoded, \"Count Vectorization\")\n",
    "\n",
    "# Word2Vec (CBOW)\n",
    "train_and_evaluate(X_train_w2v_cbow, X_valid_w2v_cbow, y_train_encoded, y_valid_encoded, \"Word2Vec (CBOW)\")\n",
    "\n",
    "# Word2Vec (Skip-gram)\n",
    "train_and_evaluate(X_train_w2v_sg, X_valid_w2v_sg, y_train_encoded, y_valid_encoded, \"Word2Vec (Skip-gram)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF - Accuracy: 0.7742796649176102, Macro F1-score: 0.5545635255259403\n",
      "Count Vectorization - Accuracy: 0.7738193869096934, Macro F1-score: 0.558167010164971\n",
      "Word2Vec (CBOW) - Accuracy: 0.7999631777593667, Macro F1-score: 0.5948373426587558\n",
      "Word2Vec (Skip-gram) - Accuracy: 0.7960968424928657, Macro F1-score: 0.6043990408673594\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 포레스트 모델 학습 및 평가 함수\n",
    "def train_and_evaluate(X_train, X_valid, y_train, y_valid, method_name):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "    macro_f1 = f1_score(y_valid, y_valid_pred, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "    # print(classification_report(y_valid, y_valid_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# TF-IDF\n",
    "train_and_evaluate(X_train_tfidf, X_valid_tfidf, y_train_encoded, y_valid_encoded, \"TF-IDF\")\n",
    "\n",
    "# Count Vectorization\n",
    "train_and_evaluate(X_train_count, X_valid_count, y_train_encoded, y_valid_encoded, \"Count Vectorization\")\n",
    "\n",
    "# Word2Vec (CBOW)\n",
    "train_and_evaluate(X_train_w2v_cbow, X_valid_w2v_cbow, y_train_encoded, y_valid_encoded, \"Word2Vec (CBOW)\")\n",
    "\n",
    "# Word2Vec (Skip-gram)\n",
    "train_and_evaluate(X_train_w2v_sg, X_valid_w2v_sg, y_train_encoded, y_valid_encoded, \"Word2Vec (Skip-gram)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cat, 나이브 베이즈 성능 별로 였음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하드보팅, 소프트보팅, 스태킹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 로지스틱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting - Accuracy: 0.8086164043082021, Macro F1-score: 0.5455198213287042\n",
      "Soft Voting - Accuracy: 0.8273036914296235, Macro F1-score: 0.6347485982842994\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# 경고 메시지 억제\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 학습, 예측 데이터셋을 불러옵니다.\n",
    "df_train = pd.read_csv(\"../../data/train_df_1012.csv\")\n",
    "df_test = pd.read_csv(\"../../data/test_df_1012.csv\")\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# 텍스트와 레이블 분리\n",
    "X = df_train['키워드']  # 키워드 컬럼\n",
    "y = df_train['분류']  # 카테고리 컬럼\n",
    "\n",
    "# 데이터 분할 (클래스 비율 동일하게 유지)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_valid_encoded = label_encoder.transform(y_valid)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# Count Vectorization\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_valid_count = count_vectorizer.transform(X_valid)\n",
    "\n",
    "# Word2Vec 벡터화 (CBOW)\n",
    "w2v_model_cbow = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=0)\n",
    "X_train_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# Word2Vec 벡터화 (Skip-gram)\n",
    "w2v_model_sg = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=1)\n",
    "X_train_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# 로지스틱 회귀 모델 정의\n",
    "model_tfidf = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model_count = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model_w2v_cbow = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model_w2v_sg = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "# 모델 학습\n",
    "model_tfidf.fit(X_train_tfidf, y_train_encoded)\n",
    "model_count.fit(X_train_count, y_train_encoded)\n",
    "model_w2v_cbow.fit(X_train_w2v_cbow, y_train_encoded)\n",
    "model_w2v_sg.fit(X_train_w2v_sg, y_train_encoded)\n",
    "\n",
    "# 하드보팅\n",
    "def hard_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict(X_valid) for model, X_valid in zip(models, X_valid_list)]).T\n",
    "    final_predictions = []\n",
    "    for preds in predictions:\n",
    "        vote_counts = Counter(preds)\n",
    "        if len(vote_counts) == len(models):  # 다수표가 없는 경우\n",
    "            final_predictions.append(preds[1])  # Count Vectorization 모델의 값을 취함\n",
    "        else:\n",
    "            final_predictions.append(vote_counts.most_common(1)[0][0])\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 소프트보팅\n",
    "def soft_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict_proba(X_valid) for model, X_valid in zip(models, X_valid_list)])\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    final_predictions = np.argmax(avg_predictions, axis=1)\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 모델 리스트\n",
    "models = [model_tfidf, model_count, model_w2v_cbow, model_w2v_sg]\n",
    "X_valid_list = [X_valid_tfidf, X_valid_count, X_valid_w2v_cbow, X_valid_w2v_sg]\n",
    "X_train_list = [X_train_tfidf, X_train_count, X_train_w2v_cbow, X_train_w2v_sg]\n",
    "\n",
    "# 하드보팅\n",
    "hard_voting(models, X_valid_list, y_valid_encoded, \"Hard Voting\")\n",
    "\n",
    "# 소프트보팅\n",
    "soft_voting(models, X_valid_list, y_valid_encoded, \"Soft Voting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_count 모델 2번 있다 생각하고 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting - Accuracy: 0.8196630764982049, Macro F1-score: 0.6010128529755605\n",
      "Soft Voting - Accuracy: 0.8366013071895425, Macro F1-score: 0.6670766691395071\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# 경고 메시지 억제\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 학습, 예측 데이터셋을 불러옵니다.\n",
    "df_train = pd.read_csv(\"../../data/train_df_1012.csv\")\n",
    "df_test = pd.read_csv(\"../../data/test_df_1012.csv\")\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# 텍스트와 레이블 분리\n",
    "X = df_train['키워드']  # 키워드 컬럼\n",
    "y = df_train['분류']  # 카테고리 컬럼\n",
    "\n",
    "# 데이터 분할 (클래스 비율 동일하게 유지)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_valid_encoded = label_encoder.transform(y_valid)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# Count Vectorization\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_valid_count = count_vectorizer.transform(X_valid)\n",
    "\n",
    "# Word2Vec 벡터화 (CBOW)\n",
    "w2v_model_cbow = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=0)\n",
    "X_train_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# Word2Vec 벡터화 (Skip-gram)\n",
    "w2v_model_sg = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=1)\n",
    "X_train_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# 로지스틱 회귀 모델 정의\n",
    "model_tfidf = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model_count = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model_w2v_cbow = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model_w2v_sg = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "# 모델 학습\n",
    "model_tfidf.fit(X_train_tfidf, y_train_encoded)\n",
    "model_count.fit(X_train_count, y_train_encoded)\n",
    "model_w2v_cbow.fit(X_train_w2v_cbow, y_train_encoded)\n",
    "model_w2v_sg.fit(X_train_w2v_sg, y_train_encoded)\n",
    "\n",
    "# 하드보팅\n",
    "def hard_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict(X_valid) for model, X_valid in zip(models, X_valid_list)]).T\n",
    "    final_predictions = []\n",
    "    for preds in predictions:\n",
    "        vote_counts = Counter(preds)\n",
    "        if len(vote_counts) == len(models):  # 다수표가 없는 경우\n",
    "            final_predictions.append(preds[1])  # Count Vectorization 모델의 값을 취함\n",
    "        else:\n",
    "            final_predictions.append(vote_counts.most_common(1)[0][0])\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 소프트보팅\n",
    "def soft_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict_proba(X_valid) for model, X_valid in zip(models, X_valid_list)])\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    final_predictions = np.argmax(avg_predictions, axis=1)\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 모델 리스트 (model_count 모델을 두 번 포함)\n",
    "models = [model_tfidf, model_count, model_count, model_w2v_cbow, model_w2v_sg]\n",
    "X_valid_list = [X_valid_tfidf, X_valid_count, X_valid_count, X_valid_w2v_cbow, X_valid_w2v_sg]\n",
    "X_train_list = [X_train_tfidf, X_train_count, X_train_count, X_train_w2v_cbow, X_train_w2v_sg]\n",
    "\n",
    "# 하드보팅\n",
    "hard_voting(models, X_valid_list, y_valid_encoded, \"Hard Voting\")\n",
    "\n",
    "# 소프트보팅\n",
    "soft_voting(models, X_valid_list, y_valid_encoded, \"Soft Voting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. 랜포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting - Accuracy: 0.7817361686458622, Macro F1-score: 0.5680682228074191\n",
      "Soft Voting - Accuracy: 0.7930590076406149, Macro F1-score: 0.5923956868842417\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# 경고 메시지 억제\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 학습, 예측 데이터셋을 불러옵니다.\n",
    "df_train = pd.read_csv(\"../../data/train_df_1012.csv\")\n",
    "df_test = pd.read_csv(\"../../data/test_df_1012.csv\")\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# 텍스트와 레이블 분리\n",
    "X = df_train['키워드']  # 키워드 컬럼\n",
    "y = df_train['분류']  # 카테고리 컬럼\n",
    "\n",
    "# 데이터 분할 (클래스 비율 동일하게 유지)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_valid_encoded = label_encoder.transform(y_valid)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# Count Vectorization\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_valid_count = count_vectorizer.transform(X_valid)\n",
    "\n",
    "# Word2Vec 벡터화 (CBOW)\n",
    "w2v_model_cbow = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=0)\n",
    "X_train_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# Word2Vec 벡터화 (Skip-gram)\n",
    "w2v_model_sg = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=1)\n",
    "X_train_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# 랜덤 포레스트 모델 정의\n",
    "model_tfidf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model_count = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model_w2v_cbow = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model_w2v_sg = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "\n",
    "# 모델 학습\n",
    "model_tfidf.fit(X_train_tfidf, y_train_encoded)\n",
    "model_count.fit(X_train_count, y_train_encoded)\n",
    "model_w2v_cbow.fit(X_train_w2v_cbow, y_train_encoded)\n",
    "model_w2v_sg.fit(X_train_w2v_sg, y_train_encoded)\n",
    "\n",
    "# 하드보팅\n",
    "def hard_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict(X_valid) for model, X_valid in zip(models, X_valid_list)]).T\n",
    "    final_predictions = []\n",
    "    for preds in predictions:\n",
    "        vote_counts = Counter(preds)\n",
    "        if len(vote_counts) == len(models):  # 다수표가 없는 경우\n",
    "            final_predictions.append(preds[3])  # Word2Vec (Skip-gram) 모델의 값을 취함\n",
    "        else:\n",
    "            final_predictions.append(vote_counts.most_common(1)[0][0])\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 소프트보팅\n",
    "def soft_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict_proba(X_valid) for model, X_valid in zip(models, X_valid_list)])\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    final_predictions = np.argmax(avg_predictions, axis=1)\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 모델 리스트\n",
    "models = [model_tfidf, model_count, model_w2v_cbow, model_w2v_sg]\n",
    "X_valid_list = [X_valid_tfidf, X_valid_count, X_valid_w2v_cbow, X_valid_w2v_sg]\n",
    "X_train_list = [X_train_tfidf, X_train_count, X_train_w2v_cbow, X_train_w2v_sg]\n",
    "\n",
    "# 하드보팅\n",
    "hard_voting(models, X_valid_list, y_valid_encoded, \"Hard Voting\")\n",
    "\n",
    "# 소프트보팅\n",
    "soft_voting(models, X_valid_list, y_valid_encoded, \"Soft Voting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-3. 성능 좋은 3개 모델 섞어서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting - Accuracy: 0.81349535119212, Macro F1-score: 0.6312578899765534\n",
      "Soft Voting - Accuracy: 0.8390868084322931, Macro F1-score: 0.6710935194397225\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# 경고 메시지 억제\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 학습, 예측 데이터셋을 불러옵니다.\n",
    "df_train = pd.read_csv(\"../../data/train_df_1012.csv\")\n",
    "df_test = pd.read_csv(\"../../data/test_df_1012.csv\")\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# 텍스트와 레이블 분리\n",
    "X = df_train['키워드']  # 키워드 컬럼\n",
    "y = df_train['분류']  # 카테고리 컬럼\n",
    "\n",
    "# 데이터 분할 (클래스 비율 동일하게 유지)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_valid_encoded = label_encoder.transform(y_valid)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# Count Vectorization\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_valid_count = count_vectorizer.transform(X_valid)\n",
    "\n",
    "# Word2Vec 벡터화 (CBOW)\n",
    "w2v_model_cbow = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=0)\n",
    "X_train_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# Word2Vec 벡터화 (Skip-gram)\n",
    "w2v_model_sg = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=1)\n",
    "X_train_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# 모델 정의\n",
    "model_count = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model_w2v_cbow = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model_w2v_sg = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "\n",
    "# 모델 학습\n",
    "model_count.fit(X_train_count, y_train_encoded)\n",
    "model_w2v_cbow.fit(X_train_w2v_cbow, y_train_encoded)\n",
    "model_w2v_sg.fit(X_train_w2v_sg, y_train_encoded)\n",
    "\n",
    "# 하드보팅\n",
    "def hard_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict(X_valid) for model, X_valid in zip(models, X_valid_list)]).T\n",
    "    final_predictions = []\n",
    "    for preds in predictions:\n",
    "        vote_counts = Counter(preds)\n",
    "        if len(vote_counts) == len(models):  # 다수표가 없는 경우\n",
    "            final_predictions.append(preds[0])  # Logistic Regression 모델의 값을 취함\n",
    "        else:\n",
    "            final_predictions.append(vote_counts.most_common(1)[0][0])\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 소프트보팅\n",
    "def soft_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict_proba(X_valid) for model, X_valid in zip(models, X_valid_list)])\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    final_predictions = np.argmax(avg_predictions, axis=1)\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 모델 리스트\n",
    "models = [model_count, model_w2v_cbow, model_w2v_sg]\n",
    "X_valid_list = [X_valid_count, X_valid_w2v_cbow, X_valid_w2v_sg]\n",
    "\n",
    "# 하드보팅\n",
    "hard_voting(models, X_valid_list, y_valid_encoded, \"Hard Voting\")\n",
    "\n",
    "# 소프트보팅\n",
    "soft_voting(models, X_valid_list, y_valid_encoded, \"Soft Voting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting - Accuracy: 0.8337475835404584, Macro F1-score: 0.6715901126181764\n",
      "Soft Voting - Accuracy: 0.8389026972291264, Macro F1-score: 0.6739752158257886\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# 경고 메시지 억제\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 학습, 예측 데이터셋을 불러옵니다.\n",
    "df_train = pd.read_csv(\"../../data/train_df_1012.csv\")\n",
    "df_test = pd.read_csv(\"../../data/test_df_1012.csv\")\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# 텍스트와 레이블 분리\n",
    "X = df_train['키워드']  # 키워드 컬럼\n",
    "y = df_train['분류']  # 카테고리 컬럼\n",
    "\n",
    "# 데이터 분할 (클래스 비율 동일하게 유지)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_valid_encoded = label_encoder.transform(y_valid)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# Count Vectorization\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_valid_count = count_vectorizer.transform(X_valid)\n",
    "\n",
    "# Word2Vec 벡터화 (CBOW)\n",
    "w2v_model_cbow = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=0)\n",
    "X_train_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_cbow = np.array([np.mean([w2v_model_cbow.wv[word] for word in text.split() if word in w2v_model_cbow.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# Word2Vec 벡터화 (Skip-gram)\n",
    "w2v_model_sg = Word2Vec(sentences=[text.split() for text in X_train], vector_size=300, window=10, min_count=2, workers=4, sg=1)\n",
    "X_train_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_train])\n",
    "X_valid_w2v_sg = np.array([np.mean([w2v_model_sg.wv[word] for word in text.split() if word in w2v_model_sg.wv] or [np.zeros(300)], axis=0) for text in X_valid])\n",
    "\n",
    "# 모델 정의\n",
    "model_count = LogisticRegression(random_state=RANDOM_STATE)\n",
    "model_w2v_cbow = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model_w2v_sg = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "\n",
    "# 모델 학습\n",
    "model_count.fit(X_train_count, y_train_encoded)\n",
    "model_w2v_cbow.fit(X_train_w2v_cbow, y_train_encoded)\n",
    "model_w2v_sg.fit(X_train_w2v_sg, y_train_encoded)\n",
    "\n",
    "# 하드보팅\n",
    "def hard_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict(X_valid) for model, X_valid in zip(models, X_valid_list)]).T\n",
    "    final_predictions = []\n",
    "    for preds in predictions:\n",
    "        vote_counts = Counter(preds)\n",
    "        if len(vote_counts) == len(models):  # 다수표가 없는 경우\n",
    "            final_predictions.append(preds[0])  # Logistic Regression 모델의 값을 취함\n",
    "        else:\n",
    "            final_predictions.append(vote_counts.most_common(1)[0][0])\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 소프트보팅\n",
    "def soft_voting(models, X_valid_list, y_valid, method_name):\n",
    "    predictions = np.array([model.predict_proba(X_valid) for model, X_valid in zip(models, X_valid_list)])\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    final_predictions = np.argmax(avg_predictions, axis=1)\n",
    "    accuracy = accuracy_score(y_valid, final_predictions)\n",
    "    macro_f1 = f1_score(y_valid, final_predictions, average='macro')\n",
    "    print(f\"{method_name} - Accuracy: {accuracy}, Macro F1-score: {macro_f1}\")\n",
    "\n",
    "# 모델 리스트\n",
    "models = [model_count, model_w2v_cbow]\n",
    "X_valid_list = [X_valid_count, X_valid_w2v_cbow, X_valid_w2v_sg]\n",
    "\n",
    "# 하드보팅\n",
    "hard_voting(models, X_valid_list, y_valid_encoded, \"Hard Voting\")\n",
    "\n",
    "# 소프트보팅\n",
    "soft_voting(models, X_valid_list, y_valid_encoded, \"Soft Voting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting - Accuracy: 0.8337475835404584, Macro F1-score: 0.6715901126181764\n",
      "Soft Voting - Accuracy: 0.8328270275246249, Macro F1-score: 0.6689213593541946\n"
     ]
    }
   ],
   "source": [
    "# 모델 리스트\n",
    "models = [model_count, model_w2v_sg]\n",
    "X_valid_list = [X_valid_count, X_valid_w2v_cbow, X_valid_w2v_sg]\n",
    "\n",
    "# 하드보팅\n",
    "hard_voting(models, X_valid_list, y_valid_encoded, \"Hard Voting\")\n",
    "\n",
    "# 소프트보팅\n",
    "soft_voting(models, X_valid_list, y_valid_encoded, \"Soft Voting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
