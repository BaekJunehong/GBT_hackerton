{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 한글 폰트 지정\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../../data/train_df_translate_del.csv\")\n",
    "test_df = pd.read_csv(\"../../data/test_df_translate_del.csv\")  \n",
    "submission = pd.read_csv(\"../../data/sample_submission.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23405\n",
      "23405\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame에서 NaN 값이 있는 행들:\n",
      "Empty DataFrame\n",
      "Columns: [ID, 분류, 제목, 키워드]\n",
      "Index: []\n",
      "\n",
      "Test DataFrame에서 NaN 값이 있는 행들:\n",
      "Empty DataFrame\n",
      "Columns: [ID, 제목, 키워드]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# NaN 값을 찾는 함수\n",
    "def find_nan_values(df, column_name):\n",
    "    nan_rows = df[df[column_name].isna()]\n",
    "    return nan_rows\n",
    "\n",
    "# train_df와 test_df에서 '키워드' 열의 NaN 값 찾기\n",
    "nan_train = find_nan_values(train_df, '키워드')\n",
    "nan_test = find_nan_values(test_df, '키워드')\n",
    "\n",
    "print(\"Train DataFrame에서 NaN 값이 있는 행들:\")\n",
    "print(nan_train)\n",
    "\n",
    "print(\"\\nTest DataFrame에서 NaN 값이 있는 행들:\")\n",
    "print(nan_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어로만 구성된 단어에 대한 처리 (주소 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 제거할 단어 리스트\n",
    "remove_words = ['http', 'www.', '.kr', '.net', '.com']\n",
    "\n",
    "def remove_specific_words(text):\n",
    "    words = text.split(',')\n",
    "    filtered_words = [word for word in words if not any(remove_word in word for remove_word in remove_words)]\n",
    "    return ','.join(filtered_words)\n",
    "\n",
    "# '키워드' 열에 함수 적용\n",
    "train_df['키워드'] = train_df['키워드'].apply(remove_specific_words)\n",
    "test_df['키워드'] = test_df['키워드'].apply(remove_specific_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23405\n",
      "23405\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame 상위 10개의 결과:\n",
      "AI       3608\n",
      "SK       3319\n",
      "GTX      2956\n",
      "LH       2919\n",
      "IC       1842\n",
      "SNS      1644\n",
      "CCTV     1532\n",
      "GH       1510\n",
      "KLPGA    1410\n",
      "TF       1386\n",
      "dtype: int64\n",
      "\n",
      "Test DataFrame 상위 10개의 결과:\n",
      "AI       1575\n",
      "SK       1358\n",
      "GTX      1275\n",
      "LH       1255\n",
      "GH        836\n",
      "IC        776\n",
      "SNS       717\n",
      "TF        619\n",
      "ESG       613\n",
      "KLPGA     579\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 영어로만 된 단어를 필터링하는 함수\n",
    "def is_english(word):\n",
    "    return bool(re.match('^[a-zA-Z]+$', word))\n",
    "\n",
    "# 영어로만 된 단어를 추출하는 함수\n",
    "def extract_english_keywords(keywords):\n",
    "    words = keywords.split(',')\n",
    "    english_words = [word for word in words if is_english(word)]\n",
    "    return english_words\n",
    "\n",
    "# 모든 영어 단어를 하나의 리스트로 모으기\n",
    "train_english_words = []\n",
    "test_english_words = []\n",
    "\n",
    "train_df['키워드'].apply(lambda x: train_english_words.extend(extract_english_keywords(x)))\n",
    "test_df['키워드'].apply(lambda x: test_english_words.extend(extract_english_keywords(x)))\n",
    "\n",
    "# 리스트를 시리즈로 변환하여 value_counts 호출\n",
    "train_english_word_counts = pd.Series(train_english_words).value_counts()\n",
    "test_english_word_counts = pd.Series(test_english_words).value_counts()\n",
    "\n",
    "# 2개 이하로 등장하는 단어 제거\n",
    "train_words_to_remove = train_english_word_counts[train_english_word_counts <= 2].index.tolist()\n",
    "test_words_to_remove = test_english_word_counts[test_english_word_counts <= 2].index.tolist()\n",
    "\n",
    "# 원본 데이터에서 해당 단어들을 제거하는 함수\n",
    "def remove_infrequent_words(keywords, words_to_remove):\n",
    "    words = keywords.split(',')\n",
    "    filtered_words = [word for word in words if word not in words_to_remove]\n",
    "    return ','.join(filtered_words)\n",
    "\n",
    "# 원본 데이터에 반영\n",
    "train_df['키워드'] = train_df['키워드'].apply(lambda x: remove_infrequent_words(x, train_words_to_remove))\n",
    "test_df['키워드'] = test_df['키워드'].apply(lambda x: remove_infrequent_words(x, test_words_to_remove))\n",
    "\n",
    "# 상위 10개의 결과 출력\n",
    "filtered_train_english_word_counts = train_english_word_counts[train_english_word_counts > 2]\n",
    "filtered_test_english_word_counts = test_english_word_counts[test_english_word_counts > 2]\n",
    "\n",
    "print(\"Train DataFrame 상위 10개의 결과:\")\n",
    "print(filtered_train_english_word_counts.head(10))\n",
    "\n",
    "print(\"\\nTest DataFrame 상위 10개의 결과:\")\n",
    "print(filtered_test_english_word_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23405\n",
      "23405\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(submission))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어, 한글 혼합 단어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 제거할 단어 리스트\n",
    "remove_words = [\n",
    "    'http', 'www.', '.kr', '.net', '.com',\n",
    "    'A씨', 'B씨', 'A양', 'C씨', 'C양', 'D씨', 'A군', 'A사', 'B군', 'B사', 'F팀', 'A업체', 'C노선',\n",
    "    'B양', 'D등급', 'L자형', 'A의원', 'E씨', 'A그룹', 'A타입', 'C등급', 'B그룹', 'A교사', 'C법인',\n",
    "    'A선수', 'A법인', 'B등급', 'B노선', 'B타입', 'J씨', 'A시행사', 'A대표', 'G사', 'E등급', 'A경위',\n",
    "    'B법인', 'R구역', 'A시의원', 'P사', 'K씨', 'C그룹', 'D센터', 'B업체', 'A교회', 'D사', 'H사', 'A아파트',\n",
    "    'B학생', 'C값', 'F씨', 'S씨', 'C군', 'H공인중개사사무소', 'A캐피탈', 'S등급', 'A고등학교', 'A경사', 'B교회',\n",
    "    'D법인', 'C사', 'G씨', 'A골프장', 'F노선', 'D노선', 'm가량', 'A팀장', 'L씨', '플랜B', 'B의원', 'A어린이집',\n",
    "    'S사', 'A농장', 'B지구', 'D구역', 'A산후조리원', 'L사', 'A노래방', 'A사령관', 'Y씨', 'B교사', 'A동', 'C의원',\n",
    "    'A원장', 'A카페', 'A연구소', 'E주식회사', 'B아파트', 'B어린이집', 'B경장', 'A요양병원', 'F채널', 'A학생',\n",
    "    'A연습장', 't가량', 'A학원', 'A건설사', 'B동', 'B초등학교', 'B건설사', 'B시장', 'n차', 'p차', 'A식품접객업소',\n",
    "    'C법', 'B한약국', 'C한약방', 'E군', 't트럭', 'G모사', 'A회사', 'A경찰관', 'A기업', 'B학원', 'C업체',\n",
    "    'A공인중개사사무소', 'A도시', 'A회장', 'V그룹', 'A고교', 'B동물병원', 'A요양원', 'SA등급', 'B도의원',\n",
    "    'C변호사', 'B시 B씨네', 'A마트', 'C병원', 'A병원', 'D아파트', 'B공인중개사', 'B선수', 'A협회', 'A사장',\n",
    "    'D양', 'B개발', 'A조합', 'E노선', '원x2.5', 'A구', 'G그룹', 'S자', 'A공', 'A검사', 'n곤충.kr', 'A팀', '0반',\n",
    "    'C중학교', 'P차이', 'km가량', 'B조', 'J건설사', 'C컨벤션', 'D건설', 'SS이슈', '시즌A', '시즌B', '시즌C', 'A농협',\n",
    "    'A약국', 'C약국', 'A구역', '존x', 'A은행', 'kw급', 'D군', 'A헬스장', 'B팀', 'C예식장', 'A단지', 'A지점', 'A지자체',\n",
    "    't급', 'M교회', 'C블록', 'B목사', '플랜C', 'B번', '개XX', 'D교사', 'B골프장', 'H건설', 'A종합부동산금융사',\n",
    "    'B세무서장', 'H산업', 'T골프장', 'H씨', 'A강사', 'B강사', 'B원장', 'J기업', 'M타워', 'A현장', 'A소방관', 'C동',\n",
    "    'F양', 'A초등학생', 'A언론사', 'A당원', 'A시', 'C상병', 'C교수', 'C학원', 'C아파트', 'A등', 'X새끼', 'D구'\n",
    "]\n",
    "\n",
    "def remove_exact_words(text):\n",
    "    words = text.split(',')\n",
    "    filtered_words = [word for word in words if word not in remove_words]\n",
    "    return ','.join(filtered_words)\n",
    "\n",
    "# '키워드' 열에 함수 적용\n",
    "train_df['키워드'] = train_df['키워드'].apply(remove_exact_words)\n",
    "test_df['키워드'] = test_df['키워드'].apply(remove_exact_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23405\n",
      "23405\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame 상위 10개의 결과:\n",
      "SK하이닉스      4057\n",
      "GS건설         511\n",
      "DL이앤씨        323\n",
      "CJ대한통운       273\n",
      "QR코드         269\n",
      "D2블록         224\n",
      "HL디앤아이한라     214\n",
      "R&D센터        200\n",
      "LG전자         199\n",
      "SK에코플랜트      191\n",
      "dtype: int64\n",
      "\n",
      "Test DataFrame 상위 10개의 결과:\n",
      "SK하이닉스      1548\n",
      "GS건설         257\n",
      "DL이앤씨        133\n",
      "CJ대한통운       127\n",
      "QR코드         114\n",
      "GTX-A노선      101\n",
      "D2블록          86\n",
      "R&D센터         85\n",
      "HL디앤아이한라      82\n",
      "A등급           81\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 영어와 한글이 혼합된 단어를 필터링하는 함수\n",
    "def is_mixed_korean_english(word):\n",
    "    return bool(re.search('[a-zA-Z]', word) and re.search('[가-힣]', word))\n",
    "\n",
    "# 영어와 한글이 혼합된 단어를 추출하는 함수\n",
    "def extract_mixed_keywords(keywords):\n",
    "    words = keywords.split(',')\n",
    "    mixed_words = [word for word in words if is_mixed_korean_english(word)]\n",
    "    return mixed_words\n",
    "\n",
    "# 모든 영어와 한글이 혼합된 단어를 하나의 리스트로 모으기\n",
    "train_mixed_words = []\n",
    "test_mixed_words = []\n",
    "\n",
    "train_df['키워드'].apply(lambda x: train_mixed_words.extend(extract_mixed_keywords(x)))\n",
    "test_df['키워드'].apply(lambda x: test_mixed_words.extend(extract_mixed_keywords(x)))\n",
    "\n",
    "# 리스트를 시리즈로 변환하여 value_counts 호출\n",
    "train_mixed_word_counts = pd.Series(train_mixed_words).value_counts()\n",
    "test_mixed_word_counts = pd.Series(test_mixed_words).value_counts()\n",
    "\n",
    "# 3개 이하로 등장하는 단어 제거\n",
    "train_words_to_remove = train_mixed_word_counts[train_mixed_word_counts <= 3].index.tolist()\n",
    "test_words_to_remove = test_mixed_word_counts[test_mixed_word_counts <= 3].index.tolist()\n",
    "\n",
    "# 원본 데이터에서 해당 단어들을 제거하는 함수\n",
    "def remove_infrequent_words(keywords, words_to_remove):\n",
    "    words = keywords.split(',')\n",
    "    filtered_words = [word for word in words if word not in words_to_remove]\n",
    "    return ','.join(filtered_words)\n",
    "\n",
    "# 원본 데이터에 반영\n",
    "train_df['키워드'] = train_df['키워드'].apply(lambda x: remove_infrequent_words(x, train_words_to_remove))\n",
    "test_df['키워드'] = test_df['키워드'].apply(lambda x: remove_infrequent_words(x, test_words_to_remove))\n",
    "\n",
    "# 상위 10개의 결과 출력\n",
    "filtered_train_mixed_word_counts = train_mixed_word_counts[train_mixed_word_counts > 3]\n",
    "filtered_test_mixed_word_counts = test_mixed_word_counts[test_mixed_word_counts > 3]\n",
    "\n",
    "print(\"Train DataFrame 상위 10개의 결과:\")\n",
    "print(filtered_train_mixed_word_counts.head(10))\n",
    "\n",
    "print(\"\\nTest DataFrame 상위 10개의 결과:\")\n",
    "print(filtered_test_mixed_word_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23405\n",
      "23405\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "제거된 단어들:\n",
      "['18일', '15일', '7월', '8월', '50%', '22일', '20명', '10명', '70명', '36종류']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 불용어 제거\n",
    "removed_keywords = []\n",
    "\n",
    "def remove_invalid_keywords(keywords):\n",
    "    pattern = re.compile(\n",
    "        r'[0-9]+[가-힣\\u4E00-\\u9FFF]+|'     # 숫자+한글\n",
    "        r'[0-9]+[A-Za-z]+|'                 # 숫자+영어\n",
    "        r'[가-힣]+[0-9]+|'                  # 한글+숫자\n",
    "        r'[A-Za-z]+[0-9]+|'                 # 영어+숫자\n",
    "\n",
    "        r'[\\u4E00-\\u9FFF]+|'                # 한자\n",
    "        r'[0-9]+(\\.[0-9]+)?%|'              # 숫자+퍼센트\n",
    "        r'[0-9]+'                          # 숫자\n",
    "    )\n",
    "    valid_keywords = []\n",
    "    exceptions = ['코로나19', 'RE100', 'GS25']\n",
    "    for word in keywords.split(','):\n",
    "        word = word.strip()\n",
    "        if not word or pattern.match(word):  # 공백이거나 패턴에 맞는 단어 제거\n",
    "            if word not in exceptions:  # 예외 처리\n",
    "                removed_keywords.append(word)\n",
    "            else:\n",
    "                valid_keywords.append(word)\n",
    "        else:\n",
    "            valid_keywords.append(word)\n",
    "    return ', '.join(valid_keywords)\n",
    "\n",
    "# 원본 데이터에서 해당 키워드들을 제거\n",
    "train_df['키워드'] = train_df['키워드'].apply(remove_invalid_keywords)\n",
    "test_df['키워드'] = test_df['키워드'].apply(remove_invalid_keywords)\n",
    "\n",
    "# 제거된 단어들 출력\n",
    "print(\"\\n제거된 단어들:\")\n",
    "print(removed_keywords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'경기용'과 '전문' 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '경기용'과 '전문' 단어 제거 함수\n",
    "def remove_keywords(keywords, words_to_remove):\n",
    "    keywords_list = keywords.split(', ')\n",
    "    filtered_keywords = [word for word in keywords_list if word not in words_to_remove]\n",
    "    return ', '.join(filtered_keywords)\n",
    "\n",
    "# 제거할 단어 목록\n",
    "words_to_remove = ['경기용', '전문']\n",
    "\n",
    "# '키워드' 열 업데이트\n",
    "train_df['키워드'] = train_df['키워드'].apply(lambda x: remove_keywords(x, words_to_remove))\n",
    "test_df['키워드'] = test_df['키워드'].apply(lambda x: remove_keywords(x, words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'김동성, 양육비, 배드파더스, 등재, 평생, 꼬리표, 인민정, 호소, 김동성, 쇼트, 트랙, 국가, 대표, 극단적, 선택, 인민정, 여자, 친구, 인민정, 자신, SNS, 심경, 인스타그램, 저, ᆼ글맘, 자신, 처지, 공개, 우울증, 공황, 장애, 김동성, 공평, 상황들, 뇌리, 장문, 김동성, ᅥᆫ만원정도, 살아야, 로, 고, 그, 꼬리표는, 평생, 달고, 사, 토로, 김동성씨, 방송, ᅳᆯ, 도, 아무것, ᅡᄀ, 스, 써, 여, ᆷᄒ, ᅩᄅ, 데, 극단적, ᅳᆯ, 할, 수밖에, 어, 바닥, 어, 이, 지옥, 상황, 결과, 호소, 김동성, 지난달, 용인시, 상현동, 자택, 극단적, 선택, 시도, 신고, 출동, 대원, 병원, 이송, 생명, 지장, ᅦᄀ, ᅳᆫ, 질타, 않으면, ᆯᄋ, ᅳᆫ데, 저는, 제가, 눈을, 뜨고, 있는, 한, 어떠한, 상황에서도, 발벗고, 나서지, 아, 일분이, 가, 이, 이, 이, 소중하기ᄋ, ᅳᆫ, 야, 설명, 배드파더스, 경제활동, 악의적, 양육비, 비양육자, 채찍질, 소득, 수입, 재산, 비양육자, 배드파더스, 낙인, 합리적, 개인적, 생각, 아내, 이혼, 양육비, 논란, 김동성, TV조선, 이혼, 프로그램, 출연, 인민정, 인스타, 그램, 싱글맘, 양육자와, 비양육자, ᆼ글맘입니다, 지금껏, 양육비ᄂ, 이엄마입니다, 도, ᅳᆯ, 걸까, 이, 이, 서, 이, 벌었습니다, 도, 아, ᅳᆯ, ᅳᆯ, 김동성씨ᄂ, 고, 랴, 로, 이, ᅥᆫ만원정도입니다, 도, 로, 고, 그, 꼬리표는, 평생, 달고, 사, 남자친구, ᅦᄀ, ᅳᆫ, 고, 느, ᆯᄋ, ᅳᆫ데, 저는, 제가, 눈을, 뜨고, 있는한, 어떠한, 상황에서도, 발벗고, 나서지, 않으면, 지금, 우리는, 올ᄉ, ᆼ입니다, 가, 이, 이, 이, ᅳᆫ, 없습니다, 도, 가야합ᄂ, 에, ᅳᆫ, ᅳᆯ, 보며, 공평하지, 못한, 이, 사, 이, ᅳᆯ, 후, 아, ᅳᆯ, 못하고, 이, 과거, 도ᄃ, 했느, ᅡᄀ, 스, 써, 여, ᆷᄒ, ᅩᄅ, 없었던, 이, 이, 으, ᆩᄋ, 어, 바닥으로, 곳ᄋ, 이, 어, ᅳᆫ, 지옥이였습니다, 이런상황ᄋ, ᅪ인걸까요, 생각입니다, 겨, 으, 도, ᅳᆯ, 로, 아, ᅳᆯ, 하되, 아무런, 소득도, 수입도, 재산도, 없는, 비ᄋ, ᅳᆯ, ᅳᆫ, 이, 배드파더스지혜롭게현명하게, 배드파더스ᄀ, ᅦᄀ, 겨, 으, 서, 펴'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확히 일치하는 행 필터링\n",
    "filtered_df = train_df[train_df['키워드'].str.contains(r'ᅳᆯ', na=False)]\n",
    "\n",
    "# 결과 출력\n",
    "filtered_df.iloc[0]['키워드']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 행 제거\n",
    "train_df = train_df.drop(filtered_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23405\n",
      "23405\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df saved as train_df_1006.csv\n",
      "test_df saved as test_df_1006.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 현재 날짜 가져오기 (MMDD 형식)\n",
    "current_date = datetime.now().strftime('%m%d')\n",
    "\n",
    "# CSV 파일명 생성\n",
    "train_csv_filename = f\"train_df_{current_date}.csv\"\n",
    "test_csv_filename = f\"test_df_{current_date}.csv\"\n",
    "\n",
    "# CSV 파일로 저장\n",
    "train_df.to_csv(train_csv_filename, index=False, encoding='utf-8-sig')\n",
    "test_df.to_csv(test_csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"train_df saved as {train_csv_filename}\")\n",
    "print(f\"test_df saved as {test_csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
